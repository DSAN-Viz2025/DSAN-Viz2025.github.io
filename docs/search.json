[
  {
    "objectID": "llm-usage.html",
    "href": "llm-usage.html",
    "title": "DSAN-Viz Mini Project 2025",
    "section": "",
    "text": "hey"
  },
  {
    "objectID": "appendix.html",
    "href": "appendix.html",
    "title": "DSAN-Viz Mini Project 2025",
    "section": "",
    "text": "test2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DSAN-Viz Mini Project 2025",
    "section": "",
    "text": "test"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "DSAN-Viz Mini Project 2025",
    "section": "",
    "text": "https://www.youtube.com/watch?v=_VGJIPRGTy4&ab_channel=PositPBC\ncolor palette: https://colormagic.app/palette/671c38d353d1e2a13ca7ee06\nimages: https://www.thenewhumanitarian.org/feature/2021/2/25/then-and-now-25-years-of-aid-worker-insecurity"
  },
  {
    "objectID": "data_cleaning.html",
    "href": "data_cleaning.html",
    "title": "Data Cleaning",
    "section": "",
    "text": "Source: The data for this analysis was sourced from the Aid Worker Security Database (Humanitarian Outcomes 2025)."
  },
  {
    "objectID": "data_cleaning.html#data-source-description",
    "href": "data_cleaning.html#data-source-description",
    "title": "Data Cleaning",
    "section": "",
    "text": "Source: The data for this analysis was sourced from the Aid Worker Security Database (Humanitarian Outcomes 2025)."
  },
  {
    "objectID": "data_cleaning.html#load-and-preview-data",
    "href": "data_cleaning.html#load-and-preview-data",
    "title": "Data Cleaning",
    "section": "Load and Preview Data",
    "text": "Load and Preview Data\n\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nplt.rcParams[\"font.family\"] = \"Tahoma\"\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\n\n# Load data\ndf = pd.read_csv(\"data/security_incidents.csv\")\ndf.head()\n\n\n\n\n\n\n\n\nIncident ID\nYear\nMonth\nDay\nCountry Code\nCountry\nRegion\nDistrict\nCity\nUN\n...\nAttack context\nLocation\nLatitude\nLongitude\nMotive\nActor type\nActor name\nDetails\nVerified\nSource\n\n\n\n\n0\n1\n1997\n1.0\nNaN\nKH\nCambodia\nBanteay Meanchey\nNaN\nNaN\n0\n...\nUnknown\nUnknown\n14.070929\n103.099916\nUnknown\nUnknown\nUnknown\n1 ICRC national staff killed while working in ...\nArchived\nArchived\n\n\n1\n2\n1997\n1.0\nNaN\nRW\nRwanda\nNorthern\nMusanze\nRuhengeri\n0\n...\nRaid\nOffice/compound\n-1.499840\n29.634970\nUnknown\nUnknown\nUnknown\n3 INGO international (Spanish) staff killed, 1...\nArchived\nArchived\n\n\n2\n3\n1997\n2.0\nNaN\nTJ\nTajikistan\nNaN\nNaN\nNaN\n4\n...\nUnknown\nUnknown\n38.628173\n70.815654\nNaN\nUnknown\nUnknown\n3 UN national staff, 1 UN international (Niger...\nArchived\nArchived\n\n\n3\n4\n1997\n2.0\nNaN\nSO\nSomalia\nLower Juba\nKismayo\nKismayo\n0\n...\nUnknown\nUnknown\n-0.358216\n42.545087\nPolitical\nNon-state armed group: Regional\nAl-Itihaad al-Islamiya\n1 INGO international staff killed by Al ittiha...\nArchived\nArchived\n\n\n4\n5\n1997\n2.0\n14.0\nRW\nRwanda\nKigali\nKigali\nKigali\n1\n...\nIndividual attack\nUnknown\n-1.950851\n30.061508\nPolitical\nUnknown\nUnknown\n1 UN national staff shot and killed in Kigali ...\nArchived\nArchived\n\n\n\n\n5 rows × 41 columns"
  },
  {
    "objectID": "data_cleaning.html#standardization",
    "href": "data_cleaning.html#standardization",
    "title": "Data Cleaning",
    "section": "Standardization",
    "text": "Standardization\nTo prepare the dataset for analysis, column headers and country names were standardized by formatting text consistently and manually correcting common acronyms or naming variations.\n\n# Headers to exclude from header standardization\nexclude = [\"UN\", \"INGO\", \"ICRC\", \"NRCS and IFRC\", \"NNGO\"]\n\n# Modify headers: lowercase and replace spaces with underscores, except for the excluded ones\ndf.columns = [\n    header.lower().replace(\" \", \"_\") if header not in exclude else header\n    for header in df.columns\n]\n\n# Title case + remove leading/trailing whitespace\ndf[\"country\"] = df[\"country\"].str.title().str.strip()\ndf[\"region\"] = df[\"region\"].str.title().str.strip()\n\n# Manual overrides for known acronyms or special cases (This was done retroactively)\nfix_countries = {\n    \"Dr Congo\": \"DR Congo\",\n    \"Cote D'Ivoire\": \"Côte d'Ivoire\",\n    \"Central African Rep.\": \"Central African Republic\",\n    \"Central African Rep\": \"Central African Republic\",\n    \"Iran, Islamic Republic Of\": \"Iran\",\n    \"Libyan Arab Jamahiriya\": \"Libya\",\n    \"Dominican Rep.\": \"Dominican Republic\",\n    \"Dominican Rep\": \"Dominican Republic\",\n}\n\ndf[\"country\"] = df[\"country\"].replace(fix_countries)\ndf.head(1)\n\n\n\n\n\n\n\n\nincident_id\nyear\nmonth\nday\ncountry_code\ncountry\nregion\ndistrict\ncity\nUN\n...\nattack_context\nlocation\nlatitude\nlongitude\nmotive\nactor_type\nactor_name\ndetails\nverified\nsource\n\n\n\n\n0\n1\n1997\n1.0\nNaN\nKH\nCambodia\nBanteay Meanchey\nNaN\nNaN\n0\n...\nUnknown\nUnknown\n14.070929\n103.099916\nUnknown\nUnknown\nUnknown\n1 ICRC national staff killed while working in ...\nArchived\nArchived\n\n\n\n\n1 rows × 41 columns"
  },
  {
    "objectID": "data_cleaning.html#handling-missing-values",
    "href": "data_cleaning.html#handling-missing-values",
    "title": "Data Cleaning",
    "section": "Handling Missing Values",
    "text": "Handling Missing Values\n\nIdentifying Missing Data\nMissing values are identified to ensure critical data gaps are addressed before proceeding with analysis.\n\nprint(\n    \"Number of rows:\", df.shape[0], \"\\n\\nMissing Data: \"\n)  #  # Understanding the total count helps in assessing the impact of missing values and evaluating the significance of data removal decisions.\nna_counts = df.isna().sum()\nprint(na_counts)\n\nNumber of rows: 4337 \n\nMissing Data: \nincident_id                   0\nyear                          0\nmonth                        48\nday                         380\ncountry_code                 33\ncountry                       5\nregion                      367\ndistrict                    736\ncity                        988\nUN                            0\nINGO                          0\nICRC                          9\nNRCS and IFRC                 9\nNNGO                          9\nother                         0\nnationals_killed              0\nnationals_wounded             0\nnationals_kidnapped           0\ntotal_nationals               0\ninternationals_killed         0\ninternationals_wounded        0\ninternationals_kidnapped      0\ntotal_internationals          0\ntotal_killed                  0\ntotal_wounded                 0\ntotal_kidnapped               0\ntotal_affected                0\ngender_male                   0\ngender_female                 0\ngender_unknown                0\nmeans_of_attack               0\nattack_context                0\nlocation                      0\nlatitude                     13\nlongitude                    13\nmotive                        4\nactor_type                    0\nactor_name                    8\ndetails                       0\nverified                      1\nsource                        0\ndtype: int64\n\n\n\n\nVisualizing Missing Data\n\nimport missingno as msno\n\nmsno.matrix(df)\n\n\n\n\n\n\n\n\n\n\nExploring Missing Data Patterns\nExploring patterns in missing data is important for identifying dependencies between variables and making better cleaning decisions. The matrix heatmap above shows where values are missing across the dataset. The correlation heatmap below goes further, showing which fields tend to be missing together. This helps avoid careless deletions or assumptions when handling missing data, and supports a more informed cleanup process.\n\nmsno.heatmap(df)\n\n\n\n\n\n\n\n\nThe missing data in this dataset appears to be Missing at Random (MAR), which means the likelihood of a value being missing is related to other observed variables in the dataset, rather than the missing value itself. Evidence for this includes the observed dependencies between fields with missing data.\n\ncity and district demonstrate a strong correlation in missing data, suggesting that when one is missing, the other is likely to be missing as well.\nThere are moderate correlations between city and region, as well as district and region, which indicates that missing data in these fields often occur together, reflecting challenges in documenting comprehensive geographic details in some records.\nBoth latitude and longitude show a strong correlation with each other and moderate correlation with country_code and country, which could imply that missing geographic coordinates are related to missing or incorrect country coding.\nThe missing day data, while not strongly correlated with any single geographic field, is often concurrently missing when region, district, and city data are absent, a pattern that underscores the challenges in recording accurate and complete information in chaotic environments.\n\n\n\nDropping Data\nData Dropping Decisions 1. Dropping rows where both latitude and longitude are missing ensures that all entries have necessary data for location-specific analysis. Interestingly, this step also removed all entries missing the country field, which aligns with ensuring geographic accuracy. Given that only 13 records were affected, this represents a minor portion (approximately 0.29%) of the dataset, minimizing the impact on the overall data integrity and availability for analysis.\n\nDropping rows where both day and month are missing because complete date information is important for temporal patterns and analysis. As in conflict zones, immediate crisis response can prioritize medical aid over detailed record-keeping, ensuring the availability of at least partial date information helps in maintaining a minimal level of temporal analysis capability.\nConsidering the chaotic environments in which this data is collected, the simultaneous absence of multiple key pieces of information, day, region, district, city in the same record could indicate recording challenges and also potentially lower reliability or raise uncertainty in these specific entries. Further, given the correlation among these fields and their collective importance for any geographic or temporal analysis, dropping these rows was necessary to enhance the reliability of the dataset.\n\n\n# Drop rows where latitude, longitude are missing\ndf.dropna(subset=[\"latitude\", \"longitude\"], how=\"all\", inplace=True)\n\n# Drop rows where day and month are missing\ndf.dropna(subset=[\"day\", \"month\"], inplace=True)\n\n# Drop rows where day, region, district, and city are all missing\ndf.dropna(subset=[\"day\", \"region\", \"district\", \"city\"], how=\"all\", inplace=True)\n\n# Print the number of rows after dropping\nprint(\"Number of rows after dropping specified missing data:\", df.shape[0], \"\\n\")\n\nNumber of rows after dropping specified missing data: 3949 \n\n\n\n\n\nFilling in Missing Data\n\nGeographic Data using Nominatim\nReverse geocoding through GeoPy’s Nominatim API will be used to populate missing geographic data in which there are missing values in the city, district, and region fields but latitude and longitude coordinates are available.\nFirst, a boolean mask is created to identify rows where at least one of the location fields is null while valid latitude and longitude values are present which allows the reverse geocoding process to run only on rows that have coordinates available for lookup.\n\nmask = (\n    df[[\"city\", \"district\", \"region\"]].isna().any(axis=1)\n    & df[\"latitude\"].notna()\n    & df[\"longitude\"].notna()\n)\n\nprint(\"Rows with missing city/district/region but with valid lat/long:\", mask.sum())\n\nRows with missing city/district/region but with valid lat/long: 762\n\n\n\nfrom geopy.geocoders import Nominatim\nfrom time import sleep\n\ngeolocator = Nominatim(user_agent=\"geo_cleaner_25\")\n\n\ndef reverse_geocode(lat, lon):\n    try:\n        location = geolocator.reverse(\n            (lat, lon), timeout=10, language=\"en\"\n        )  # force English\n        if location and location.raw.get(\"address\"):\n            address = location.raw[\"address\"]\n            return {\n                \"city\": address.get(\"city\")\n                or address.get(\"town\")\n                or address.get(\"village\"),\n                \"district\": address.get(\"county\"),\n                \"region\": address.get(\"state\"),\n            }\n    except:\n        return None\n\n\n# Apply to missing rows (throttle with sleep to avoid being blocked)\nfilled = []\nfor idx, row in df[mask].iterrows():\n    result = reverse_geocode(row[\"latitude\"], row[\"longitude\"])\n    if result:\n        for key in [\"city\", \"district\", \"region\"]:\n            if pd.isna(df.at[idx, key]) and result.get(key):\n                df.at[idx, key] = result[key]\n    sleep(1)  # Nominatim rate limit\n\n\nprint(\"Remaining missing values:\")\nprint(df[[\"city\", \"district\", \"region\"]].isna().sum())\n\nupdated_mask = mask & df[[\"city\", \"district\", \"region\"]].notna().any(axis=1)\n\nprint(\"Rows successfully updated with at least one field:\", updated_mask.sum())\n\nRemaining missing values:\ncity        264\ndistrict    178\nregion       15\ndtype: int64\nRows successfully updated with at least one field: 761\n\n\n\n# Fill remaining geographic missing data with Unknown\ndf[\"district\"].fillna(\"Unknown\", inplace=True)\ndf[\"city\"].fillna(\"Unknown\", inplace=True)\ndf[\"region\"].fillna(\"Unknown\", inplace=True)\n\n\nStandardizing Newly Added Geographic Data\nTo standardize newly added geographic names, accented characters were replaced with their unaccented equivalents using a custom mapping function. This step helps avoid inconsistencies caused by special characters in the dataset. Quotation marks and commas were also removed from location fields to prevent parsing issues and ensure consistent text formatting.\n\n# Dictionary to replace accented characters with plain equivalents\nreplacements = {\n    \"é\": \"e\",\n    \"è\": \"e\",\n    \"ê\": \"e\",\n    \"á\": \"a\",\n    \"í\": \"i\",\n    \"ó\": \"o\",\n    \"ú\": \"u\",\n    \"ñ\": \"n\",\n}\n\n\n# Replace accented characters in each cell\ndef replace_accents(text):\n    if isinstance(text, str):\n        for accented_char, replacement in replacements.items():\n            text = text.replace(accented_char, replacement)\n        return text\n    return text\n\n\ndf = df.applymap(replace_accents)\n\n# Characters to remove from location fields\nunwanted_chars = ['\"', \",\"]\n\n\n# Remove unwanted punctuation from strings\ndef clean_location_text(text):\n    if isinstance(text, str):\n        for char in unwanted_chars:\n            text = text.replace(char, \"\")\n    return text\n\n\n# Clean only city, district, and region columns\nfor col in [\"city\", \"district\", \"region\"]:\n    df[col] = df[col].apply(clean_location_text)\n\n\n\n\nCountry Codes\n\nmissing_country_code = df[df[[\"country_code\"]].isna().all(axis=1)]\nprint(\"Unique countries where 'country_code' is missing:\")\nprint(missing_country_code[\"country\"].unique())\n\nUnique countries where 'country_code' is missing:\n['Chechnya' 'Namibia']\n\n\nSource: ISO Country Codes\n\ncountry_code_mapping = {\"Namibia\": \"NA\", \"Chechnya\": \"RU-CE\"}\n\ndf.loc[df[\"country\"].isin([\"Namibia\", \"Chechnya\"]), \"country_code\"] = df[\"country\"].map(\n    country_code_mapping\n)\n\n\n\nRemaining Missing Data\n\ndf[\"motive\"].fillna(\"Unknown\", inplace=True)\ndf[\"actor_name\"].fillna(\"Unknown\", inplace=True)\ndf[\"verified\"].fillna(\n    \"No\", inplace=True\n)  # If not \"yes\", \"archived\", or \"pending\" which are the available values then the assumption is no, there is only one NA for \"Verified\" so this feels safe.\n\n# Check remaining missing data summary\nprint(df.isna().sum())\n\nincident_id                 0\nyear                        0\nmonth                       0\nday                         0\ncountry_code                0\ncountry                     0\nregion                      0\ndistrict                    0\ncity                        0\nUN                          0\nINGO                        0\nICRC                        9\nNRCS and IFRC               9\nNNGO                        9\nother                       0\nnationals_killed            0\nnationals_wounded           0\nnationals_kidnapped         0\ntotal_nationals             0\ninternationals_killed       0\ninternationals_wounded      0\ninternationals_kidnapped    0\ntotal_internationals        0\ntotal_killed                0\ntotal_wounded               0\ntotal_kidnapped             0\ntotal_affected              0\ngender_male                 0\ngender_female               0\ngender_unknown              0\nmeans_of_attack             0\nattack_context              0\nlocation                    0\nlatitude                    0\nlongitude                   0\nmotive                      0\nactor_type                  0\nactor_name                  0\ndetails                     0\nverified                    0\nsource                      0\ndtype: int64\n\n\nNow there are only three remaining variables with missing data: ICRC, NRCS and IFRC, and NNGO. Examination of these fields revealed that all three fields are simultaneously missing in 9 instances.\nTo maintain dataset integrity without dropping potentially valuable data, a placeholder will be introduced with a value of 100 for these missing entries. This method allows for the retention of these records in the analysis while clearly marking them for easy exclusion during analytical phases where accurate incident counts are necessary.\nHere are the unique values currently in these fields:\n\nprint(\"Unique values in 'ICRC':\", df[\"ICRC\"].unique())\nprint(\"Unique values in 'NRCS and IFRC':\", df[\"NRCS and IFRC\"].unique())\nprint(\"Unique values in 'NNGO':\", df[\"NNGO\"].unique())\n\nUnique values in 'ICRC': [ 0.  1.  7.  6.  2.  4. nan  3.  8.  5.]\nUnique values in 'NRCS and IFRC': [ 0.  3.  4.  1.  2.  6. 14. nan 15.  7. 11.  5. 10. 18. 19.  9.]\nUnique values in 'NNGO': [ 0.  3.  1.  4.  5.  2.  8. nan 12.  6.  7. 10. 11. 14.  9. 15.]\n\n\n\n# Fill missing values with 100 for specific columns\ndf[\"ICRC\"] = df[\"ICRC\"].fillna(100)\ndf[\"NRCS and IFRC\"] = df[\"NRCS and IFRC\"].fillna(100)\ndf[\"NNGO\"] = df[\"NNGO\"].fillna(100)\n\n\n# Check remaining missing data summary\nprint(df.isna().sum())\n# Print the number of rows after dropping\nprint(\"Number of rows:\", df.shape[0], \"\\n\")\n\nincident_id                 0\nyear                        0\nmonth                       0\nday                         0\ncountry_code                0\ncountry                     0\nregion                      0\ndistrict                    0\ncity                        0\nUN                          0\nINGO                        0\nICRC                        0\nNRCS and IFRC               0\nNNGO                        0\nother                       0\nnationals_killed            0\nnationals_wounded           0\nnationals_kidnapped         0\ntotal_nationals             0\ninternationals_killed       0\ninternationals_wounded      0\ninternationals_kidnapped    0\ntotal_internationals        0\ntotal_killed                0\ntotal_wounded               0\ntotal_kidnapped             0\ntotal_affected              0\ngender_male                 0\ngender_female               0\ngender_unknown              0\nmeans_of_attack             0\nattack_context              0\nlocation                    0\nlatitude                    0\nlongitude                   0\nmotive                      0\nactor_type                  0\nactor_name                  0\ndetails                     0\nverified                    0\nsource                      0\ndtype: int64\nNumber of rows: 3949 \n\n\n\nNow all of the missing data has been handled while maintaining the integrity of the data!"
  },
  {
    "objectID": "data_cleaning.html#data-type-optimization",
    "href": "data_cleaning.html#data-type-optimization",
    "title": "Data Cleaning",
    "section": "Data Type Optimization",
    "text": "Data Type Optimization\n\nprint(df.dtypes)\n\nincident_id                   int64\nyear                          int64\nmonth                       float64\nday                         float64\ncountry_code                 object\ncountry                      object\nregion                       object\ndistrict                     object\ncity                         object\nUN                            int64\nINGO                          int64\nICRC                        float64\nNRCS and IFRC               float64\nNNGO                        float64\nother                         int64\nnationals_killed              int64\nnationals_wounded             int64\nnationals_kidnapped           int64\ntotal_nationals               int64\ninternationals_killed         int64\ninternationals_wounded        int64\ninternationals_kidnapped      int64\ntotal_internationals          int64\ntotal_killed                  int64\ntotal_wounded                 int64\ntotal_kidnapped               int64\ntotal_affected                int64\ngender_male                   int64\ngender_female                 int64\ngender_unknown                int64\nmeans_of_attack              object\nattack_context               object\nlocation                     object\nlatitude                    float64\nlongitude                   float64\nmotive                       object\nactor_type                   object\nactor_name                   object\ndetails                      object\nverified                     object\nsource                       object\ndtype: object\n\n\n\n# Convert fields that represent counts to int\ndf[\"ICRC\"] = df[\"ICRC\"].astype(int)\ndf[\"NRCS and IFRC\"] = df[\"NRCS and IFRC\"].astype(int)\ndf[\"NNGO\"] = df[\"NNGO\"].astype(int)\n\n\nDate Conversion and Creating DateTime\nTo streamline analysis, separate year, month, and day columns were combined into a single date column using pd.to_datetime(). These columns were then dropped to keep the dataset tidy.\n\n# Convert to integers\ndf[\"year\"] = df[\"year\"].astype(int)\ndf[\"month\"] = df[\"month\"].astype(int)\ndf[\"day\"] = df[\"day\"].astype(int)\n\n# New date column from year, month, and day\ndf[\"date\"] = pd.to_datetime(df[[\"year\", \"month\", \"day\"]])\n\ndf.drop([\"year\", \"month\", \"day\"], axis=1, inplace=True)\n\n\n# Move date column after incident_id and before country_code\ncols = df.columns.tolist()\n\n# Remove date from current position\ncols.remove(\"date\")\n\n# Find index of incident_id\nidx = cols.index(\"incident_id\") + 1\ncols.insert(idx, \"date\")\n\n# Reorder the dataframe\ndf = df[cols]\ndf.head()\n\n\n\n\n\n\n\n\nincident_id\ndate\ncountry_code\ncountry\nregion\ndistrict\ncity\nUN\nINGO\nICRC\n...\nattack_context\nlocation\nlatitude\nlongitude\nmotive\nactor_type\nactor_name\ndetails\nverified\nsource\n\n\n\n\n4\n5\n1997-02-14\nRW\nRwanda\nKigali\nKigali\nKigali\n1\n0\n0\n...\nIndividual attack\nUnknown\n-1.950851\n30.061508\nPolitical\nUnknown\nUnknown\n1 UN national staff shot and killed in Kigali ...\nArchived\nArchived\n\n\n6\n6\n1997-05-07\nSL\nSierra Leone\nTonkolili District\nUnknown\nMayamba\n3\n0\n0\n...\nAmbush\nRoad\n8.640035\n-11.840027\nUnknown\nUnknown\nUnknown\n1 UN national staff driver killed and 2 UN nat...\nArchived\nArchived\n\n\n8\n12\n1997-06-08\nRW\nRwanda\nNorthern\nMusanze\nRuhengeri\n0\n1\n0\n...\nCombat/Crossfire\nUnknown\n-1.499840\n29.634970\nIncidental\nUnknown\nUnknown\n1 INGO national staff killed when he was among...\nArchived\nArchived\n\n\n9\n8\n1997-06-14\nRW\nRwanda\nNorthern\nMusanze\nRuhengeri\n1\n0\n0\n...\nIndividual attack\nHome\n-1.499840\n29.634970\nPolitical\nNon-state armed group: Unknown\nNot applicable\n1 UN national staff shot and killed in Ruhenge...\nArchived\nArchived\n\n\n10\n9\n1997-06-17\nRW\nRwanda\nNorthern\nMusanze\nRuhengeri\n1\n0\n0\n...\nDetention\nUnknown\n-1.499840\n29.634970\nPolitical\nUnknown\nUnknown\n1 UN national staff shot and killed in Ruhenge...\nArchived\nArchived\n\n\n\n\n5 rows × 39 columns"
  },
  {
    "objectID": "data_cleaning.html#save-cleaned-csv",
    "href": "data_cleaning.html#save-cleaned-csv",
    "title": "Data Cleaning",
    "section": "Save Cleaned CSV",
    "text": "Save Cleaned CSV\n\n# Save to a CSV file\ndf.to_csv(\"data/cleaned_security_incidents.csv\", index=False)"
  },
  {
    "objectID": "data_cleaning.html#check-for-duplicates",
    "href": "data_cleaning.html#check-for-duplicates",
    "title": "Data Cleaning",
    "section": "Check for Duplicates",
    "text": "Check for Duplicates\n\nif not df[\"incident_id\"].is_unique:\n    print(\"Duplicate IDs found\")\n\nThere are no duplicates in the dataset!"
  }
]